<?php

declare(strict_types=1);

namespace App\Console\Commands;

use App\Models\WebCrawl;
use App\Models\WebCrawlUrlCrawl;
use App\Services\Crawler\WebCrawlerService;
use Illuminate\Console\Command;
use Illuminate\Support\Facades\DB;
use Illuminate\Support\Facades\Http;
use Illuminate\Support\Facades\Storage;

class CrawlDiagnosticCommand extends Command
{
    protected $signature = 'crawl:diagnostic
                            {crawl_id? : ID du crawl Ã  diagnostiquer (dernier si non spÃ©cifiÃ©)}
                            {--test-url= : Tester l\'extraction de liens sur une URL spÃ©cifique}
                            {--fix : Tenter de corriger les problÃ¨mes dÃ©tectÃ©s}';

    protected $description = 'Diagnostique les problÃ¨mes d\'un crawl';

    public function handle(WebCrawlerService $crawler): int
    {
        $crawlId = $this->argument('crawl_id');
        $testUrl = $this->option('test-url');

        // Si test URL spÃ©cifique
        if ($testUrl) {
            return $this->testUrlExtraction($testUrl, $crawler);
        }

        // RÃ©cupÃ©rer le crawl
        $crawl = $crawlId
            ? WebCrawl::find($crawlId)
            : WebCrawl::latest()->first();

        if (! $crawl) {
            $this->error('Aucun crawl trouvÃ©');

            return Command::FAILURE;
        }

        $this->info("ðŸ” Diagnostic du crawl #{$crawl->id}");
        $this->newLine();

        // 1. Configuration du crawl
        $this->diagnosticConfig($crawl);

        // 2. Stats des URLs
        $this->diagnosticUrls($crawl);

        // 3. Stats de la queue
        $this->diagnosticQueue($crawl);

        // 4. DerniÃ¨res activitÃ©s
        $this->diagnosticActivity($crawl);

        // 5. Test extraction sur une URL du crawl
        $this->diagnosticExtraction($crawl, $crawler);

        // 6. Suggestions
        $this->showSuggestions($crawl);

        return Command::SUCCESS;
    }

    private function diagnosticConfig(WebCrawl $crawl): void
    {
        $this->info('ðŸ“‹ Configuration:');

        $configData = [
            ['ParamÃ¨tre', 'Valeur', 'Status'],
            ['ID', $crawl->id, ''],
            ['Status', $crawl->status, $crawl->status === 'completed' ? 'âš ï¸ TerminÃ©' : 'âœ…'],
            ['URL de dÃ©part', $crawl->start_url, ''],
            ['max_pages', $crawl->max_pages, $crawl->max_pages == 0 ? 'âœ… IllimitÃ©' : ($crawl->max_pages <= 100 ? 'âš ï¸ Limite basse!' : 'âœ…')],
            ['max_depth', $crawl->max_depth, $crawl->max_depth == 99 ? 'âœ… IllimitÃ©' : ''],
            ['pages_discovered', $crawl->pages_discovered, ''],
            ['pages_crawled', $crawl->pages_crawled, ''],
            ['DÃ©marrÃ©', $crawl->started_at?->format('d/m H:i:s') ?? '-', ''],
            ['TerminÃ©', $crawl->completed_at?->format('d/m H:i:s') ?? '-', ''],
        ];

        $this->table(['ParamÃ¨tre', 'Valeur', 'Status'], array_slice($configData, 1));

        // Alerte si max_pages semble Ãªtre le problÃ¨me
        if ($crawl->max_pages > 0 && $crawl->pages_discovered >= $crawl->max_pages) {
            $this->error("â›” PROBLÃˆME DÃ‰TECTÃ‰: pages_discovered ({$crawl->pages_discovered}) >= max_pages ({$crawl->max_pages})");
            $this->line("   â†’ Le crawl s'est arrÃªtÃ© car la limite de pages a Ã©tÃ© atteinte.");
            $this->line("   â†’ Solution: Ã‰ditez le crawl et mettez max_pages Ã  0 (IllimitÃ©)");
        }

        $this->newLine();
    }

    private function diagnosticUrls(WebCrawl $crawl): void
    {
        $this->info('ðŸ“Š Statistiques des URLs:');

        $stats = WebCrawlUrlCrawl::where('crawl_id', $crawl->id)
            ->selectRaw('status, COUNT(*) as count')
            ->groupBy('status')
            ->pluck('count', 'status')
            ->toArray();

        $depthStats = WebCrawlUrlCrawl::where('crawl_id', $crawl->id)
            ->selectRaw('depth, COUNT(*) as count')
            ->groupBy('depth')
            ->orderBy('depth')
            ->pluck('count', 'depth')
            ->toArray();

        $this->line('   Par statut:');
        foreach ($stats as $status => $count) {
            $icon = match ($status) {
                'fetched' => 'âœ…',
                'pending' => 'â³',
                'fetching' => 'ðŸ”„',
                'error' => 'âŒ',
                default => 'â“',
            };
            $this->line("     {$icon} {$status}: {$count}");
        }

        $this->newLine();
        $this->line('   Par profondeur:');
        foreach ($depthStats as $depth => $count) {
            $bar = str_repeat('â–ˆ', min($count, 50));
            $this->line("     Depth {$depth}: {$count} {$bar}");
        }

        // VÃ©rifier si toutes les URLs sont Ã  max_depth
        $maxDepthInCrawl = max(array_keys($depthStats) ?: [0]);
        if ($crawl->max_depth != 99 && $maxDepthInCrawl >= $crawl->max_depth) {
            $atMaxDepth = $depthStats[$crawl->max_depth] ?? 0;
            if ($atMaxDepth > 0) {
                $this->warn("   âš ï¸ {$atMaxDepth} URLs sont Ã  la profondeur max ({$crawl->max_depth})");
                $this->line("      â†’ Ces URLs ne gÃ©nÃ©reront pas de nouveaux liens");
            }
        }

        // VÃ©rifier les erreurs
        $errors = WebCrawlUrlCrawl::where('crawl_id', $crawl->id)
            ->where('status', 'error')
            ->limit(5)
            ->with('url')
            ->get();

        if ($errors->isNotEmpty()) {
            $this->newLine();
            $this->error('   Exemples d\'erreurs:');
            foreach ($errors as $entry) {
                $this->line("     - {$entry->url?->url}");
                $this->line("       Message: {$entry->error_message}");
            }
        }

        $this->newLine();
    }

    private function diagnosticQueue(WebCrawl $crawl): void
    {
        $this->info('ðŸ“¬ Ã‰tat de la queue:');

        // Jobs en attente
        $pendingJobs = DB::table('jobs')->count();
        $failedJobs = DB::table('failed_jobs')->count();

        // Jobs liÃ©s Ã  ce crawl (approximatif)
        $crawlJobs = DB::table('jobs')
            ->where('payload', 'like', '%CrawlUrlJob%')
            ->count();

        $this->line("   Jobs en attente (total): {$pendingJobs}");
        $this->line("   Jobs CrawlUrlJob: {$crawlJobs}");
        $this->line("   Jobs Ã©chouÃ©s: {$failedJobs}");

        if ($failedJobs > 0) {
            $this->warn("   âš ï¸ Il y a des jobs Ã©chouÃ©s - vÃ©rifiez avec: php artisan queue:failed");
        }

        if ($crawl->status === 'running' && $crawlJobs === 0 && ($stats['pending'] ?? 0) === 0) {
            $this->error("   â›” Le crawl est 'running' mais aucun job en queue!");
            $this->line("      â†’ Possible race condition dans checkCrawlCompletion");
        }

        $this->newLine();
    }

    private function diagnosticActivity(WebCrawl $crawl): void
    {
        $this->info('ðŸ“ˆ ActivitÃ© rÃ©cente:');

        // DerniÃ¨res URLs crawlÃ©es
        $recentUrls = WebCrawlUrlCrawl::where('crawl_id', $crawl->id)
            ->whereNotNull('fetched_at')
            ->orderBy('fetched_at', 'desc')
            ->limit(5)
            ->with('url')
            ->get();

        if ($recentUrls->isEmpty()) {
            $this->line('   Aucune URL crawlÃ©e rÃ©cemment');
        } else {
            $this->line('   DerniÃ¨res URLs crawlÃ©es:');
            foreach ($recentUrls as $entry) {
                $time = $entry->fetched_at->format('H:i:s');
                $url = \Illuminate\Support\Str::limit($entry->url?->url ?? 'N/A', 60);
                $this->line("     [{$time}] {$url}");
            }
        }

        $this->newLine();
    }

    private function diagnosticExtraction(WebCrawl $crawl, WebCrawlerService $crawler): void
    {
        $this->info('ðŸ”— Test d\'extraction de liens:');

        // Prendre une URL fetched au hasard
        $sampleEntry = WebCrawlUrlCrawl::where('crawl_id', $crawl->id)
            ->where('status', 'fetched')
            ->whereHas('url', fn ($q) => $q->whereNotNull('storage_path'))
            ->with('url')
            ->first();

        if (! $sampleEntry || ! $sampleEntry->url) {
            $this->line('   Aucune URL avec contenu stockÃ© trouvÃ©e');

            return;
        }

        $url = $sampleEntry->url;
        $this->line("   URL testÃ©e: {$url->url}");
        $this->line("   Content-Type: {$url->content_type}");
        $this->line("   Storage path: {$url->storage_path}");

        // VÃ©rifier si le fichier existe
        if (! Storage::disk('local')->exists($url->storage_path)) {
            $this->error("   â›” Le fichier n'existe pas sur le disque!");

            return;
        }

        // Charger le contenu et extraire les liens
        $content = Storage::disk('local')->get($url->storage_path);
        $this->line('   Taille contenu: ' . strlen($content) . ' bytes');

        if (str_contains($url->content_type ?? '', 'text/html')) {
            $links = $crawler->extractLinks($content, $url->url);
            $this->line('   Liens extraits: ' . count($links));

            if (count($links) > 0) {
                $this->line('   Ã‰chantillon (5 premiers):');
                foreach (array_slice($links, 0, 5) as $link) {
                    $this->line("     - {$link}");
                }
            } else {
                $this->warn('   âš ï¸ Aucun lien extrait - le site utilise peut-Ãªtre JavaScript');
            }

            // VÃ©rifier combien seraient filtrÃ©s par domaine
            $allowedDomains = array_filter(explode("\n", $crawl->allowed_domains ?? ''));
            if (empty($allowedDomains)) {
                $parsed = parse_url($crawl->start_url);
                $allowedDomains = [$parsed['host'] ?? ''];
            }

            $filteredCount = 0;
            foreach ($links as $link) {
                $linkHost = parse_url($link, PHP_URL_HOST);
                $allowed = false;
                foreach ($allowedDomains as $domain) {
                    if ($linkHost === $domain || str_ends_with($linkHost, '.' . $domain)) {
                        $allowed = true;
                        break;
                    }
                }
                if (! $allowed) {
                    $filteredCount++;
                }
            }

            if ($filteredCount > 0) {
                $this->line("   Liens filtrÃ©s (domaine externe): {$filteredCount}");
            }
        }

        $this->newLine();
    }

    private function showSuggestions(WebCrawl $crawl): void
    {
        $this->info('ðŸ’¡ Suggestions:');

        $suggestions = [];

        // VÃ©rifier max_pages
        if ($crawl->max_pages > 0 && $crawl->pages_discovered >= $crawl->max_pages) {
            $suggestions[] = "Le crawl est limitÃ© Ã  {$crawl->max_pages} pages. Modifiez le crawl pour mettre 'IllimitÃ©'";
        }

        // VÃ©rifier si terminÃ© prÃ©maturÃ©ment
        if ($crawl->status === 'completed' && $crawl->pages_discovered < 100) {
            $suggestions[] = 'Le crawl s\'est terminÃ© avec peu de pages. VÃ©rifiez les logs pour "Web crawl completed"';
        }

        // VÃ©rifier la queue
        $pendingJobs = DB::table('jobs')->count();
        if ($pendingJobs === 0 && $crawl->status === 'running') {
            $suggestions[] = 'Aucun job en queue mais crawl "running". Relancez le worker ou corrigez le status';
        }

        // VÃ©rifier les failed jobs
        $failedJobs = DB::table('failed_jobs')->count();
        if ($failedJobs > 0) {
            $suggestions[] = "Il y a {$failedJobs} jobs Ã©chouÃ©s. ExÃ©cutez: php artisan queue:failed";
        }

        if (empty($suggestions)) {
            $this->line('   âœ… Aucun problÃ¨me Ã©vident dÃ©tectÃ©');
            $this->line('   â†’ VÃ©rifiez les logs: tail -f storage/logs/laravel.log | grep -E "(crawl|CrawlUrlJob)"');
        } else {
            foreach ($suggestions as $i => $suggestion) {
                $num = $i + 1;
                $this->line("   {$num}. {$suggestion}");
            }
        }

        $this->newLine();
    }

    private function testUrlExtraction(string $url, WebCrawlerService $crawler): int
    {
        $this->info("ðŸ” Test d'extraction pour: {$url}");
        $this->newLine();

        try {
            // Fetch l'URL
            $this->line('Fetching...');
            $response = Http::timeout(30)
                ->withHeaders(['User-Agent' => 'Mozilla/5.0 (compatible; Googlebot/2.1)'])
                ->get($url);

            $this->line("Status: {$response->status()}");
            $this->line('Content-Type: ' . ($response->header('Content-Type') ?? 'N/A'));
            $this->line('Content-Length: ' . strlen($response->body()) . ' bytes');

            if (! $response->successful()) {
                $this->error('Ã‰chec du fetch');

                return Command::FAILURE;
            }

            // Extraire les liens
            $links = $crawler->extractLinks($response->body(), $url);

            $this->newLine();
            $this->info('ðŸ“Š RÃ©sultats:');
            $this->line('Liens trouvÃ©s: ' . count($links));

            if (count($links) > 0) {
                $this->newLine();
                $this->line('Liens (20 premiers):');
                foreach (array_slice($links, 0, 20) as $link) {
                    $this->line("  - {$link}");
                }
            }

            // Analyser les liens par domaine
            $domains = [];
            foreach ($links as $link) {
                $host = parse_url($link, PHP_URL_HOST) ?? 'unknown';
                $domains[$host] = ($domains[$host] ?? 0) + 1;
            }

            if (! empty($domains)) {
                $this->newLine();
                $this->line('Par domaine:');
                arsort($domains);
                foreach (array_slice($domains, 0, 10, true) as $domain => $count) {
                    $this->line("  {$domain}: {$count}");
                }
            }

        } catch (\Exception $e) {
            $this->error('Erreur: ' . $e->getMessage());

            return Command::FAILURE;
        }

        return Command::SUCCESS;
    }
}
